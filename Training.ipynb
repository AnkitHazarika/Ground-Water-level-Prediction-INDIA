{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load And Process DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV\n",
    "df = pd.read_csv(\"groundwater_yearly_averages.csv\")\n",
    "\n",
    "# Drop rows with missing important values\n",
    "df.dropna(subset=['Station_name', 'lat', 'long', 'Year', 'level'], inplace=True)\n",
    "\n",
    "# Encode station name\n",
    "station_encoder = LabelEncoder()\n",
    "df['Station_ID'] = station_encoder.fit_transform(df['Station_name'])\n",
    "\n",
    "# Features: station ID, lat, long, year\n",
    "X = df[['Station_ID', 'lat', 'long', 'Year']].values\n",
    "y = df['level'].values\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "338/338 [==============================] - 2s 6ms/step - loss: 48.9967 - mae: 3.5896 - val_loss: 100.4046 - val_mae: 3.5718\n",
      "Epoch 2/50\n",
      "338/338 [==============================] - 2s 5ms/step - loss: 44.3707 - mae: 3.2518 - val_loss: 98.9617 - val_mae: 3.4658\n",
      "Epoch 3/50\n",
      "338/338 [==============================] - 2s 6ms/step - loss: 43.6680 - mae: 3.1849 - val_loss: 98.2951 - val_mae: 3.5307\n",
      "Epoch 4/50\n",
      "338/338 [==============================] - 2s 5ms/step - loss: 43.4243 - mae: 3.1655 - val_loss: 98.5773 - val_mae: 3.2870\n",
      "Epoch 5/50\n",
      "338/338 [==============================] - 2s 5ms/step - loss: 43.2595 - mae: 3.1382 - val_loss: 98.5854 - val_mae: 3.6073\n",
      "Epoch 6/50\n",
      "338/338 [==============================] - 2s 6ms/step - loss: 43.1101 - mae: 3.1415 - val_loss: 98.1657 - val_mae: 3.4008\n",
      "Epoch 7/50\n",
      "338/338 [==============================] - 2s 5ms/step - loss: 42.9449 - mae: 3.1167 - val_loss: 99.0856 - val_mae: 3.8788\n",
      "Epoch 8/50\n",
      "338/338 [==============================] - 2s 5ms/step - loss: 43.0431 - mae: 3.1418 - val_loss: 97.6094 - val_mae: 3.3352\n",
      "Epoch 9/50\n",
      "338/338 [==============================] - 2s 5ms/step - loss: 42.7257 - mae: 3.1091 - val_loss: 97.4574 - val_mae: 3.3355\n",
      "Epoch 10/50\n",
      "338/338 [==============================] - 2s 5ms/step - loss: 42.6300 - mae: 3.0995 - val_loss: 97.5918 - val_mae: 3.2604\n",
      "Epoch 11/50\n",
      "338/338 [==============================] - 2s 5ms/step - loss: 42.5270 - mae: 3.0942 - val_loss: 97.1206 - val_mae: 3.3593\n",
      "Epoch 12/50\n",
      "338/338 [==============================] - 2s 5ms/step - loss: 42.3378 - mae: 3.0802 - val_loss: 97.0886 - val_mae: 3.4729\n",
      "Epoch 13/50\n",
      "338/338 [==============================] - 2s 5ms/step - loss: 42.1786 - mae: 3.0811 - val_loss: 96.8653 - val_mae: 3.3377\n",
      "Epoch 14/50\n",
      "338/338 [==============================] - 2s 5ms/step - loss: 42.1387 - mae: 3.0669 - val_loss: 96.6063 - val_mae: 3.3150\n",
      "Epoch 15/50\n",
      "338/338 [==============================] - 2s 5ms/step - loss: 42.1764 - mae: 3.0733 - val_loss: 96.5813 - val_mae: 3.2995\n",
      "Epoch 16/50\n",
      "338/338 [==============================] - 2s 5ms/step - loss: 41.8927 - mae: 3.0525 - val_loss: 96.2458 - val_mae: 3.3727\n",
      "Epoch 17/50\n",
      "338/338 [==============================] - 2s 5ms/step - loss: 41.7358 - mae: 3.0505 - val_loss: 96.1828 - val_mae: 3.3153\n",
      "Epoch 18/50\n",
      "338/338 [==============================] - 2s 5ms/step - loss: 41.5125 - mae: 3.0337 - val_loss: 95.9047 - val_mae: 3.2623\n",
      "Epoch 19/50\n",
      "338/338 [==============================] - 2s 5ms/step - loss: 41.4338 - mae: 3.0274 - val_loss: 95.6082 - val_mae: 3.2882\n",
      "Epoch 20/50\n",
      "338/338 [==============================] - 2s 6ms/step - loss: 41.3079 - mae: 3.0147 - val_loss: 96.1470 - val_mae: 3.2562\n",
      "Epoch 21/50\n",
      "338/338 [==============================] - 2s 6ms/step - loss: 41.0754 - mae: 3.0059 - val_loss: 95.6406 - val_mae: 3.2941\n",
      "Epoch 22/50\n",
      "338/338 [==============================] - 2s 6ms/step - loss: 41.1105 - mae: 3.0170 - val_loss: 96.6504 - val_mae: 3.1541\n",
      "Epoch 23/50\n",
      "338/338 [==============================] - 2s 6ms/step - loss: 40.9922 - mae: 2.9973 - val_loss: 95.4149 - val_mae: 3.2410\n",
      "Epoch 24/50\n",
      "338/338 [==============================] - 2s 5ms/step - loss: 40.7430 - mae: 2.9783 - val_loss: 95.0132 - val_mae: 3.2302\n",
      "Epoch 25/50\n",
      "338/338 [==============================] - 2s 5ms/step - loss: 40.7100 - mae: 2.9942 - val_loss: 95.2458 - val_mae: 3.1861\n",
      "Epoch 26/50\n",
      "338/338 [==============================] - 2s 5ms/step - loss: 40.5971 - mae: 2.9774 - val_loss: 94.9287 - val_mae: 3.1798\n",
      "Epoch 27/50\n",
      "338/338 [==============================] - 2s 5ms/step - loss: 40.4818 - mae: 2.9616 - val_loss: 94.6672 - val_mae: 3.1969\n",
      "Epoch 28/50\n",
      "338/338 [==============================] - 2s 6ms/step - loss: 40.2500 - mae: 2.9620 - val_loss: 94.8094 - val_mae: 3.2005\n",
      "Epoch 29/50\n",
      "338/338 [==============================] - 2s 5ms/step - loss: 40.3639 - mae: 2.9644 - val_loss: 94.7374 - val_mae: 3.1971\n",
      "Epoch 30/50\n",
      "338/338 [==============================] - 2s 6ms/step - loss: 40.2923 - mae: 2.9561 - val_loss: 95.0234 - val_mae: 3.1918\n",
      "Epoch 31/50\n",
      "338/338 [==============================] - 2s 5ms/step - loss: 40.0805 - mae: 2.9437 - val_loss: 94.7206 - val_mae: 3.1546\n",
      "Epoch 32/50\n",
      "338/338 [==============================] - 2s 5ms/step - loss: 39.9716 - mae: 2.9470 - val_loss: 94.7417 - val_mae: 3.1716\n",
      "Epoch 33/50\n",
      "338/338 [==============================] - 2s 5ms/step - loss: 39.9963 - mae: 2.9380 - val_loss: 94.3905 - val_mae: 3.1713\n",
      "Epoch 34/50\n",
      "338/338 [==============================] - 2s 5ms/step - loss: 39.9532 - mae: 2.9477 - val_loss: 94.8082 - val_mae: 3.1357\n",
      "Epoch 35/50\n",
      "338/338 [==============================] - 2s 5ms/step - loss: 39.8355 - mae: 2.9270 - val_loss: 94.1329 - val_mae: 3.2599\n",
      "Epoch 36/50\n",
      "338/338 [==============================] - 2s 5ms/step - loss: 39.6715 - mae: 2.9127 - val_loss: 93.8858 - val_mae: 3.3183\n",
      "Epoch 37/50\n",
      "338/338 [==============================] - 2s 6ms/step - loss: 39.6614 - mae: 2.9227 - val_loss: 93.8629 - val_mae: 3.2978\n",
      "Epoch 38/50\n",
      "338/338 [==============================] - 2s 5ms/step - loss: 39.7633 - mae: 2.9471 - val_loss: 94.4148 - val_mae: 3.1195\n",
      "Epoch 39/50\n",
      "338/338 [==============================] - 2s 5ms/step - loss: 39.5878 - mae: 2.9054 - val_loss: 93.8233 - val_mae: 3.2249\n",
      "Epoch 40/50\n",
      "338/338 [==============================] - 2s 5ms/step - loss: 39.6188 - mae: 2.9297 - val_loss: 93.6987 - val_mae: 3.1787\n",
      "Epoch 41/50\n",
      "338/338 [==============================] - 2s 5ms/step - loss: 39.4931 - mae: 2.9105 - val_loss: 94.0418 - val_mae: 3.1555\n",
      "Epoch 42/50\n",
      "338/338 [==============================] - 2s 5ms/step - loss: 39.2836 - mae: 2.8837 - val_loss: 95.2593 - val_mae: 3.6935\n",
      "Epoch 43/50\n",
      "338/338 [==============================] - 2s 6ms/step - loss: 39.6432 - mae: 2.9282 - val_loss: 93.7632 - val_mae: 3.1626\n",
      "Epoch 44/50\n",
      "338/338 [==============================] - 2s 5ms/step - loss: 39.3201 - mae: 2.9059 - val_loss: 94.2721 - val_mae: 3.1796\n",
      "Epoch 45/50\n",
      "338/338 [==============================] - 2s 5ms/step - loss: 39.2574 - mae: 2.9057 - val_loss: 93.4941 - val_mae: 3.1616\n",
      "Epoch 46/50\n",
      "338/338 [==============================] - 2s 5ms/step - loss: 39.2839 - mae: 2.9052 - val_loss: 93.7377 - val_mae: 3.2079\n",
      "Epoch 47/50\n",
      "338/338 [==============================] - 2s 5ms/step - loss: 39.0350 - mae: 2.8962 - val_loss: 94.0493 - val_mae: 3.2107\n",
      "Epoch 48/50\n",
      "338/338 [==============================] - 2s 5ms/step - loss: 39.2575 - mae: 2.8988 - val_loss: 93.2251 - val_mae: 3.1013\n",
      "Epoch 49/50\n",
      "338/338 [==============================] - 2s 5ms/step - loss: 39.0042 - mae: 2.8872 - val_loss: 94.4089 - val_mae: 3.0928\n",
      "Epoch 50/50\n",
      "338/338 [==============================] - 2s 5ms/step - loss: 39.1596 - mae: 2.8977 - val_loss: 93.7607 - val_mae: 3.1111\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1)  # Output layer: one continuous value\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=50, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler.pkl']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save(\"groundwater_level_model.h5\")\n",
    "joblib.dump(station_encoder, \"station_encoder.pkl\")\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ground_level(station_name, year):\n",
    "    # Load encoders and model\n",
    "    import tensorflow as tf\n",
    "    import joblib\n",
    "    import numpy as np\n",
    "\n",
    "    station_encoder = joblib.load(\"station_encoder.pkl\")\n",
    "    scaler = joblib.load(\"scaler.pkl\")\n",
    "    model = tf.keras.models.load_model(\"groundwater_level_model.h5\")\n",
    "\n",
    "    # Get lat/long of station\n",
    "    station_row = df[df['Station_name'] == station_name].iloc[0]\n",
    "    station_id = station_encoder.transform([station_name])[0]\n",
    "\n",
    "    lat = station_row['lat']\n",
    "    long = station_row['long']\n",
    "\n",
    "    # Prepare input\n",
    "    input_data = np.array([[station_id, lat, long, year]])\n",
    "    input_scaled = scaler.transform(input_data)\n",
    "\n",
    "    # Predict\n",
    "    predicted_level = model.predict(input_scaled)[0][0]\n",
    "\n",
    "    # Print all info\n",
    "    print(f\"📍 Station: {station_name}\")\n",
    "    print(f\"🧭 Location: Latitude = {lat}, Longitude = {long}\")\n",
    "    print(f\"📅 Year: {year}\")\n",
    "    print(f\"💧 Predicted Groundwater Level: {predicted_level:.2f} m\")\n",
    "\n",
    "    return predicted_level, lat, long\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000260C72EDF30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 78ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19.131746"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_ground_level(\"Karondiya\", 2026)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
